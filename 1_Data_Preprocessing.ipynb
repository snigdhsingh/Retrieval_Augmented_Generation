{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSTALL DEPENDENT LIBRARIES\n",
    "\n",
    "\n",
    "!pip install tqdm --quiet\n",
    "!pip install pandas --quiet\n",
    "#workhorse for converting text into embeddings/vectors\n",
    "!pip install sentence-transformers==2.2.2 --quiet\n",
    "#data framework for LLM applications\n",
    "!pip install llama-index==0.9.6.post1 --quiet\n",
    "#logging output\n",
    "!pip install loguru==0.7.0 --quiet\n",
    "#convenient pretty printing library\n",
    "!pip install rich==13.7.0 --quiet\n",
    "#openai Tokenizer library\n",
    "!pip install tiktoken --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard libraries\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple, Union, Callable\n",
    "from math import ceil\n",
    "\n",
    "#external libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rich import print\n",
    "from torch import cuda\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#external files\n",
    "from preprocessing import FileIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 1 -->  Import YouTube/Podcast Transcripts\n",
    "***\n",
    "### The Data\n",
    "The data - **384** podcast episodes to be exact - we will use for this project is a collection of transcribed podcast/youtube episodes of <a href=\"https://www.youtube.com/@TomBilyeu\" target=\"_blank\">The Impact Theory</a>.  From the About section of Impact Theory:\n",
    "<h4 ><em>\"Impact Theoryâ„¢ is a weekly interview show that explores the mindsets of the world's highest achievers to learn their secrets to success.</em></h4>\n",
    "<h4 ><em>Hosted by Quest Nutrition co-founder Tom Bilyeu, Impact Theory is designed to give people the tools and knowledge they need to unlock their potential and impact the world.\"</em></h4>\n",
    "<br>\n",
    " The episodes for this course were pulled from the podcast audio files and transcribed using the OpenAI open source [Whisper model](https://github.com/openai/whisper).  The transcriptions were then linked back to each episode using a unique **video_id**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total # of episodes: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total # of episodes: \u001b[1;36m384\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "root_folder = 'data/'\n",
    "data_file = 'impact_theory_data.json'\n",
    "data_path = os.path.join(root_folder, data_file)\n",
    "\n",
    "with open(data_path) as f:\n",
    "    data =  json.load(f)\n",
    "print(f'Total # of episodes: {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/impact_theory_data.json'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line to see data for first entry, it's big\n",
    "# data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'video_id'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'playlist_id'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'length'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'thumbnail_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'views'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'episode_url'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'guest'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'summary'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'title'\u001b[0m,\n",
       "    \u001b[32m'video_id'\u001b[0m,\n",
       "    \u001b[32m'playlist_id'\u001b[0m,\n",
       "    \u001b[32m'length'\u001b[0m,\n",
       "    \u001b[32m'thumbnail_url'\u001b[0m,\n",
       "    \u001b[32m'views'\u001b[0m,\n",
       "    \u001b[32m'episode_url'\u001b[0m,\n",
       "    \u001b[32m'guest'\u001b[0m,\n",
       "    \u001b[32m'summary'\u001b[0m,\n",
       "    \u001b[32m'content'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">content : You said these are dangerous times. The world order is shifting before our eyes. We also both know that \n",
       "with hyper disruptive technologies like AI on the horizon, a good outcome is not guaranteed. Why do you think big \n",
       "tech will become the third superpower and what are the dangers and opportunities if it does? Big tech is \n",
       "essentially sovereign over the digital world. The fact that former President Trump was de-platformed from Facebook \n",
       "and from Twitter when he was president, you know, most powerful political figure on the planet. And he's just taken\n",
       "off of those networks and as a consequence, hu\n",
       "</pre>\n"
      ],
      "text/plain": [
       "content : You said these are dangerous times. The world order is shifting before our eyes. We also both know that \n",
       "with hyper disruptive technologies like AI on the horizon, a good outcome is not guaranteed. Why do you think big \n",
       "tech will become the third superpower and what are the dangers and opportunities if it does? Big tech is \n",
       "essentially sovereign over the digital world. The fact that former President Trump was de-platformed from Facebook \n",
       "and from Twitter when he was president, you know, most powerful political figure on the planet. And he's just taken\n",
       "off of those networks and as a consequence, hu\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Seeing truncated data and field names\n",
    "my_fields = []\n",
    "for key, value in data[0].items() :\n",
    "    my_fields.append(key)\n",
    "\n",
    "print (my_fields)\n",
    "print (\"content : {}\".format(data[0]['content'][0:600]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Glossary\n",
    "- **title**: Episode title.\n",
    "- **video_id**: Unique identifier for each individual episode.\n",
    "- **playlist_id**: Unique identifier for entire playlist of episodes.  All episodes in this dataset will have the same playlist_id.\n",
    "- **length**: Length of the episode in total seconds.\n",
    "- **thumbnail_url**: Hyperlink to the associated image of the YouTube episode.\n",
    "- **views**: Number of views of this YouTube episode.\n",
    "- **episode_url**: Hyperlink to the episode on YouTube.\n",
    "- **guest**: Main guest for the episode.\n",
    "- **summary**: Summary of the episode (AI generated).\n",
    "- **content**: The full transcript of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>384.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12821.268229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7650.847177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1819.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7888.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9894.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16857.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48502.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            # Words\n",
       "count    384.000000\n",
       "mean   12821.268229\n",
       "std     7650.847177\n",
       "min     1819.000000\n",
       "25%     7888.000000\n",
       "50%     9894.500000\n",
       "75%    16857.000000\n",
       "max    48502.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#content length stats\n",
    "contents = [d['content'] for d in data]\n",
    "content_lengths = [len(content.split()) for content in contents]\n",
    "df = pd.DataFrame(content_lengths, columns=['# Words'])\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "In the above example we split our content on whitespace which effectively splits the entire episode into a list of words....but LLMs operate on tokens. Tokens are words or sub-parts of words, so the word \"texting\" might be broken into two tokens `text` and `##ing`. Punctuation are also given their own tokens, so a period, a comma, or an apostropher are all going to have their own tokenized versions.  A [750 word document will be about 1000 tokens](https://www.anyscale.com/blog/num-every-llm-developer-should-know), in other words the token to word ratio is roughly 1.3 tokens = 1 word. Let's see what this looks like in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The mean word count for each episode is about <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12822</span> words, which corresponds to a rough token count of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16669</span> \n",
       "tokens.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The mean word count for each episode is about \u001b[1;36m12822\u001b[0m words, which corresponds to a rough token count of \u001b[1;36m16669\u001b[0m \n",
       "tokens.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_word_count = ceil(np.mean(content_lengths))\n",
    "token_to_word_ratio = 1.3\n",
    "approx_token_count = ceil(mean_word_count * token_to_word_ratio)\n",
    "print(f'The mean word count for each episode is about {mean_word_count} words, which corresponds to a rough token count of {approx_token_count} tokens.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of precision, let's actually tokenize our content and see how close we get to the **1.3** `token_to_word_ratio`.  We'll use the tokenizer for the `gpt-3.5-turbo-0613` LLM (avilable through the `tikotken` library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>384.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15516.523438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9380.049480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2210.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9523.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12007.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>20268.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>61559.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           # Tokens\n",
       "count    384.000000\n",
       "mean   15516.523438\n",
       "std     9380.049480\n",
       "min     2210.000000\n",
       "25%     9523.750000\n",
       "50%    12007.500000\n",
       "75%    20268.000000\n",
       "max    61559.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken # Awesome tokenizer library for use with OpenAI LLMs\n",
    "\n",
    "#instantiate tokenizer for use with ChatGPT-3.5-Turbo\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo-0613')\n",
    "\n",
    "tokens = encoding.encode_batch(contents)\n",
    "token_counts = list(map(len, tokens))\n",
    "token_df = pd.DataFrame(token_counts, columns=['# Tokens'])\n",
    "token_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.21"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How close to this approximate 1.3 token:word ratio were we\n",
    "true_ratio = round(np.mean(token_counts)/mean_word_count, 2)\n",
    "true_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total Tokens in Corpus: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5958345</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total Tokens in Corpus: \u001b[1;36m5958345\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's get one more metric, let's see how big (in tokens) the size of our total corpus is:\n",
    "total_tokens = sum(token_counts)\n",
    "print(f'Total Tokens in Corpus: {total_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 million tokens is no small matter, it's roughly equivalent to 10,000 pages of text, but to put things in perspective the Falcon-40B LLM was trained on a [1 trillion token](https://synthedia.substack.com/p/falcon-40b-llm-trained-on-1-trillion) corpus, which is 167,000 times larger than the one we are currently using! ðŸ¤¯\n",
    "\n",
    "Embedding Tokens vs. LLM Tokens (i.e. Retriever vs. Reader):\n",
    "\n",
    "We know that as part of our system we want to feed relevant context to an OpenAI LLM.  All LLMs, even the largest ones, have a limited input context window, so we'll have to take that into consideration as part of our system design.  But before we even get to that point, we have to first consider the maximum input sequence (in tokens) for our emebdding model.  Given that the mean token count for each of our podcast episodes is over 15K tokens, we'll need a way to chunk our content so that we can effectively embed the transcript into several meaningful embeddings as opposed to one embedding for the entire transcript. Let's address how we'll accomplish that task.\n",
    "\n",
    "## Step 2 -->  Sentence Splitting and Chunking\n",
    "***\n",
    "\n",
    "### Chunking:\n",
    "The open source embedding models that we will be working with have a `max_sequence_length` parameter far less than 15K tokens. In fact, the embedding model that we'll use: [`all-miniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2), has a default max input length of 256 tokens. In order to effectively process the content of our podcast episodes, we'll need to decide on a **Chunking Strategy**.\n",
    "\n",
    "1. Gather documents.  In our case we're considering a single Impact Theory episode to be a \"document\".\n",
    "2. Decide how to split your documents.  When working with text documents, a good starting strategy is to split on sentence boundaries.  This decision at the very least, prevents input text from randomly being cut off in the middle of a paragraph.  We'll use an excellent piece of code written by [LlamaIndex](https://github.com/run-llama/llama_index/blob/main/llama_index/node_parser/text/sentence.py) namely, the `SentenceSplitter` class.\n",
    "3. Pick a chunk size based on your use case.  Given that the `all-miniLM-L6-v2` was trained on documents no greater than 256 tokens in length, we'll start with 256.  A chunk size of 256 strikes a good balance between information granularity and not blowing up the embedding space.\n",
    "4. After splitting, a single episode will be broken up into `n` number of chunks, depending on the length of the episode.  Each of those chunks, however, should be no longer than 256 tokens.  Those chunks can then be passed through our embedding model as inputs and converted into vector representations.\n",
    "5. The final step is combining the text chunks, their associated vectors, and the original metadata into a single dictionary (one for each chunk), in preparation for follow-on indexing on our Weaviate datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.text_splitter import SentenceSplitter #one of the best on the market\n",
    "\n",
    "#set chunk size and instantiate your SentenceSplitter\n",
    "chunk_size = 256\n",
    "gpt35_txt_splitter = SentenceSplitter(chunk_size=chunk_size, tokenizer=encoding.encode, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_contents(corpus: List[dict],\n",
    "                   text_splitter: SentenceSplitter,\n",
    "                   content_field: str='content'\n",
    "                   ) -> List[List[str]]:\n",
    "\n",
    "    corpus_chunk_list = []\n",
    "    for episode in corpus:\n",
    "      output = text_splitter.split_text(episode[content_field])\n",
    "      corpus_chunk_list.append(output)\n",
    "    \n",
    "    return corpus_chunk_list\n",
    "\n",
    "content_splits = split_contents(data, gpt35_txt_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m384\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">83</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m83\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (len(content_splits))\n",
    "print (len(content_splits[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.430s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "from unitesting_utils import load_impact_theory_data\n",
    "\n",
    "class TestSplitContents(unittest.TestCase):\n",
    "    '''\n",
    "    Unit test to ensure proper functionality of split_contents function\n",
    "    '''\n",
    "\n",
    "    def test_split_contents(self):\n",
    "        import tiktoken\n",
    "        from llama_index.text_splitter import SentenceSplitter\n",
    "\n",
    "        data = load_impact_theory_data()\n",
    "\n",
    "        subset = data[:3]\n",
    "        chunk_size = 256\n",
    "        chunk_overlap = 0\n",
    "        encoding = tiktoken.encoding_for_model('gpt-3.5-turbo-0613')\n",
    "        gpt35_txt_splitter = SentenceSplitter(chunk_size=chunk_size, tokenizer=encoding.encode, chunk_overlap=chunk_overlap)\n",
    "        results = split_contents(subset, gpt35_txt_splitter)\n",
    "        self.assertEqual(len(results), 3)\n",
    "        self.assertEqual(len(results[0]), 83)\n",
    "        self.assertEqual(len(results[1]), 178)\n",
    "        self.assertEqual(len(results[2]), 144)\n",
    "        self.assertTrue(isinstance(results, list))\n",
    "        self.assertTrue(isinstance(results[0], list))\n",
    "        self.assertTrue(isinstance(results[0][0], str))\n",
    "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestSplitContents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#helper function to capture the lengths of each split\n",
    "def get_split_lengths(splits: List[List[str]], column_name: str='Split Lengths'):\n",
    "    \n",
    "    lengths = list(map(len, splits))\n",
    "    return pd.DataFrame(lengths, columns=[column_name])\n",
    "\n",
    "column_name = 'Split Lengths'\n",
    "df = get_split_lengths(content_splits, column_name=column_name)\n",
    "# reverse the order of the episode # to correctly show left to right chronological order\n",
    "df.index = sorted(list(df.index), reverse=True)\n",
    "# create plot\n",
    "ax = df.iloc[::-1].plot.bar(y='Split Lengths', xlabel='Episode #', ylabel='Approx. Length in Tokens', title='Episode Length (in tokens) over Time', figsize=(20,8))\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 -->  Encode Text Chunks as Vectors\n",
    "We are going to use the `all-MiniLM-L6-v2` variant because it's actually quite good out of the box and it also happens to be very fast (at embedding creation).  [Click here](https://www.sbert.net/docs/pretrained_models.html) for a comprehensive list of available pretrained SentenceTransformers (these are all available through the HuggingFace API).\n",
    "\n",
    "Becuase we are building an Information Retrieval system that utilizes Vector Search, we're going to be most interested in the following numbers:\n",
    "- Performance Semantic Search\n",
    "- Speed (encoding speed)\n",
    "- Model Size\n",
    "\n",
    "Ultimately model selection comes down to your use case and the trade-off between performance and latency (and to a degree the size of the model on disk). Give yourself a chance to experiment with different embedding models to see how they can boost retrieval performance in your system.  When you're ready for that step head on over to the [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) where you can check out the latest state of the art embedding models.  The `all-MiniLM-L6-v2` ranks a lowly 47th on the Retrieval task.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "#define the model you want to use\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick glance at the model configuration shows us the following information:\n",
    "- **Max sequence length:** (or input length) is 256 tokens.  The underlying model is a BERT model, so technically we could adjust this parameter to 512 tokens, however, because the model was trained on inputs of no greater than 256 tokens, we'll keep the default.\n",
    "- **The embedding dimension:** (i.e. the output vector) is 384 dimensions.  Make sure not to confuse the `max_sequence_length` with the embedding dimesion.  The maximum sequence length is the maximum number of input tokens the model will process before truncating any text over that limit, and the emnbedding dim is the length of the output vector which always remains fixed in size.\n",
    "- **Pooling Layer:** this model uses the mean token pooling strategy for creating a fixed sized embedding from all vector outputs.\n",
    "\n",
    "We are going to work solely with the `model.encode` method for our text embedding creation.  There is a surprising amount of work being done under the hood with this function to include word tokenization, batching, GPU device setting, etc.\n",
    "- `sentences`: accepts either a single string or a list of strings.  Very convenient.  This will allow us to enter all of the chunks from a single podcast episode at once, allowing for efficient processing and episode integrity.\n",
    "- `batch_size`: under the hood the list of sentences in the input is batched.  Default is 32, so if your input was 128 sentences, then the model would translate that into 4 batches for processing.  DataLoader not needed.  If you are using Google Colab for this notebook, use the default setting.\n",
    "- `convert_to_numpy`: default output is a numpy array. However, because Weaviate does not accept the np.array data type, we'll want to eventually convert these arrays into Python lists.\n",
    "- `device`: set to either cpu or GPU (`\"cuda:0\"`) for single GPU.  If `None` the function will handle this step for you and choose the fastest option!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Episode Embeddings:\n",
    "\n",
    "#### We'll write a function that will vectorize each content split list (single episode) and then merge the text content and their vector representations into a single list of (text, vector) tuples.  Gather all episode lists into a single large list. We need to preserve the ordering of the text and their vector representations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_content_splits(content_splits: List[List[str]],\n",
    "                          model: SentenceTransformer,\n",
    "                          device: str='cuda:0'\n",
    "                          ) -> List[List[Tuple[str, np.array]]]:\n",
    "    '''Encode content splits as vector embeddings from a list of content splits\n",
    "     where each list of splits is a single podcast episode.'''\n",
    "\n",
    "    text_vector_tuples = []\n",
    "    for episode in tqdm(content_splits):\n",
    "      text_vector_tuples.append(list(zip(episode,model.encode(episode))))\n",
    "\n",
    "    return text_vector_tuples\n",
    "  \n",
    "text_vector_tuples = encode_content_splits(content_splits, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data not available at /content/impact_theory_data.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e8e2fa53f843bdb9bc987b894ec879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 16.822s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unit Test to ensure you're on the right track\n",
    "import unittest\n",
    "from unitesting_utils import load_impact_theory_data\n",
    "\n",
    "class TestEncodeContentSplits(unittest.TestCase):\n",
    "    '''\n",
    "    Unit test to ensure proper functionality of split_contents function\n",
    "    '''\n",
    "\n",
    "    def test_encode_content_splits(self):\n",
    "        import tiktoken\n",
    "        from numpy import ndarray\n",
    "        from llama_index.text_splitter import SentenceSplitter\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "\n",
    "        data = load_impact_theory_data()\n",
    "\n",
    "        #get splits first\n",
    "        subset = data[:3]\n",
    "        chunk_size = 256\n",
    "        chunk_overlap = 0\n",
    "        encoding = tiktoken.encoding_for_model('gpt-3.5-turbo-0613')\n",
    "        gpt35_txt_splitter = SentenceSplitter(chunk_size=chunk_size, tokenizer=encoding.encode, chunk_overlap=chunk_overlap)\n",
    "        splits = split_contents(subset, gpt35_txt_splitter)\n",
    "\n",
    "        #encode splits\n",
    "        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        results = encode_content_splits(splits, model)\n",
    "\n",
    "        #run assertion tests\n",
    "        self.assertEqual(len(results), 3)\n",
    "        self.assertEqual(len(results[0]), 83)\n",
    "        self.assertEqual(len(results[1]), 178)\n",
    "        self.assertEqual(len(results[2]), 144)\n",
    "        self.assertTrue(isinstance(results, list))\n",
    "        self.assertTrue(isinstance(results[0], list))\n",
    "        self.assertTrue(isinstance(results[0][0], tuple))\n",
    "        self.assertTrue(isinstance(results[0][0][0], str))\n",
    "        self.assertTrue(isinstance(results[0][0][1], ndarray))\n",
    "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestEncodeContentSplits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Metadata with Text/Vectors\n",
    "\n",
    "#### *We will write a function that will combine episode metadata from the original corpus with their associated text/vector tuples*\n",
    "#### We don't want our metadata to go to waste.  Now that we have our text/vector tuples grouped by episode, we can easily join all of that information together with each episode's metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_metadata(corpus: List[dict],\n",
    "                  text_vector_list: List[List[Tuple[str, np.array]]],\n",
    "                  content_field: str='content',\n",
    "                  embedding_field: str='content_embedding'\n",
    "                 ) -> List[dict]:\n",
    "    '''\n",
    "    Combine episode metadata from original corpus with text/vectors tuples.\n",
    "    Creates a new dictionary for each text/vector combination.\n",
    "    '''\n",
    "\n",
    "    joined_documents = []\n",
    "\n",
    "    for i, doc in enumerate(corpus):\n",
    "      for j,tup in enumerate(text_vector_list[i]):\n",
    "\n",
    "        d = {k:v for k,v in doc.items() if k!=content_field}\n",
    "\n",
    "        video_id = doc['video_id']\n",
    "        d['doc_id'] = f'{video_id}_{j}'\n",
    "        d['content'] = tup[0]\n",
    "        d['content_embedding'] = tup[1].tolist()\n",
    "\n",
    "        joined_documents.append(d)\n",
    "\n",
    "    return joined_documents\n",
    "\n",
    "docs = join_metadata(data, text_vector_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26448"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 --> Save\n",
    "\n",
    "We've reached the final step in our data preparation phase. We should have 26,448 distinct documents and now ready to index those documents on **Weaviate host/database**. Before moving on to the indexing step, we'll want to store our dataset on disk.\n",
    "\n",
    "The FileIO Class is a convenient wrapper that makes it easy to save data to disk in either json or [parquet format](https://towardsdatascience.com/demystifying-the-parquet-file-format-13adb0206705).  The choice is yours on which format to use, however, a note of caution: the json format will take up 3-4x more room on disk than parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-11 12:16:59.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpreprocessing\u001b[0m:\u001b[36msave_as_parquet\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mDataFrame saved as parquet file here: impact-theory-minilmL6-256.parquet\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#instantiate FileIO Class\n",
    "io = FileIO()\n",
    "outpath = \"impact-theory-minilmL6-256.parquet\"\n",
    "io.save_as_parquet(file_path=outpath, data=docs, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (26448, 12)\n",
      "Memory Usage: 2.42+ MB\n"
     ]
    }
   ],
   "source": [
    "#Verify that you can reload data\n",
    "data_with_vectors = io.load_parquet(file_path=outpath)\n",
    "# data_with_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9acfe90dd9363e37394870b40236d751d0922ea7aa12f35ac9a39888e799f85c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
